{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtuSrazlNYEL"
      },
      "source": [
        "# Copyright\n",
        "\n",
        "<PRE>\n",
        "Copyright (c) Bálint Gyires-Tóth - All Rights Reserved\n",
        "You may use and modify this code for research and development purpuses.\n",
        "Using this code for educational purposes (self-paced or instructor led) without the permission of the author is prohibited.\n",
        "</PRE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vriXNd_nL2q6"
      },
      "source": [
        "# Assignment: RNN text generation with your favorite book\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5atve1sMH9n"
      },
      "source": [
        "## 1. Dataset\n",
        "- Download your favorite book from https://www.gutenberg.org/\n",
        "- Combine all sonnets into a single text source.  \n",
        "- Split into training (80%) and validation (20%).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvKdt5EyMDug",
        "outputId": "30275457-7832-47b6-ef20-c3295e9c8cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-23 18:35:57--  https://www.gutenberg.org/cache/epub/41/pg41.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 90938 (89K) [text/plain]\n",
            "Saving to: ‘pg41.txt’\n",
            "\n",
            "pg41.txt            100%[===================>]  88.81K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-04-23 18:35:58 (1.14 MB/s) - ‘pg41.txt’ saved [90938/90938]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# download\n",
        "!wget https://www.gutenberg.org/cache/epub/41/pg41.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g4WT4ex-euzQ"
      },
      "outputs": [],
      "source": [
        "with open('pg41.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "start = \"*** START OF THE PROJECT GUTENBERG EBOOK THE LEGEND OF SLEEPY HOLLOW ***\"\n",
        "end = \"*** END OF THE PROJECT GUTENBERG EBOOK THE LEGEND OF SLEEPY HOLLOW ***\"\n",
        "text = text[text.find(start)+len(start):text.rfind(end)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split = int(0.8 * len(text))\n",
        "train_text = text[:split]\n",
        "val_text = text[split:]"
      ],
      "metadata": {
        "id": "doW-eH9A7y6i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eQMcyPgMLJ9"
      },
      "source": [
        "## 2. Preprocessing\n",
        "- Convert text to lowercase.  \n",
        "- Remove punctuation (except basic sentence delimiters).  \n",
        "- Tokenize by words or characters (your choice).  \n",
        "- Build a vocabulary (map each unique word to an integer ID)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "RvXRFVcbMLe9"
      },
      "outputs": [],
      "source": [
        "# lowercase\n",
        "train_text = train_text.lower()\n",
        "val_text = val_text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7LHAFB8pqH_",
        "outputId": "c3a0bd82-84e4-497a-e52d-1512e19ebf36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the legend of sleepy hollow by washington irving found among the papers of the late diedrich knicker\n",
            "ooded glen known by the name of wileys swamp. a few rough logs laid side by side served for a bridge\n"
          ]
        }
      ],
      "source": [
        "# remove punctuation (keep words, whitespace, .?!)\n",
        "clean_train_text = re.sub(r'[^\\w\\s.?!]', '', train_text)\n",
        "clean_train_text = re.sub(r'\\n', ' ', clean_train_text)\n",
        "clean_train_text = re.sub(r'\\s+', ' ', clean_train_text)\n",
        "clean_train_text = clean_train_text.strip()\n",
        "clean_val_text = re.sub(r'[^\\w\\s.?!]', '', val_text)\n",
        "clean_val_text = re.sub(r'\\n', ' ', clean_val_text)\n",
        "print(clean_train_text[:100])\n",
        "print(clean_val_text[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDd--tNKtvdz",
        "outputId": "344589d9-4a71-4845-fef8-544e28fb4328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# download and install the spacy language model\n",
        "!python3 -m spacy download en_core_web_sm\n",
        "sp = spacy.load('en_core_web_sm')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "NZ9IU3xivCZB"
      },
      "outputs": [],
      "source": [
        "# tokenize\n",
        "train_tokens = word_tokenize(clean_train_text)\n",
        "val_tokens = word_tokenize(clean_val_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hIZmIBqNwUbo"
      },
      "outputs": [],
      "source": [
        "train_words = set(train_tokens)\n",
        "vocab = {word: idx for idx, word in enumerate(train_words)}\n",
        "train_id_to_word = np.array(list(train_words))\n",
        "\n",
        "val_words = set(val_tokens)\n",
        "val_id_to_word = np.array(list(val_words))\n",
        "\n",
        "train_ids = [vocab[token] for token in train_tokens] # array of id's\n",
        "val_ids = [vocab[token] for token in val_tokens if token in vocab]\n",
        "\n",
        "window_size=10\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(len(train_ids) - window_size):\n",
        "    X.append(train_ids[i:i + window_size])\n",
        "    Y.append(train_ids[i + window_size])\n",
        "\n",
        "X_train=np.array(X)\n",
        "Y_train=np.array(Y)\n",
        "\n",
        "x, y = [], []\n",
        "for i in range(len(val_ids) - window_size):\n",
        "    x.append(val_ids[i:i + window_size])\n",
        "    y.append(val_ids[i + window_size])\n",
        "\n",
        "X_val = np.array(x)\n",
        "Y_val = np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbTZs3OiMMNy"
      },
      "source": [
        "## 3. Embedding Layer in Keras\n",
        "Below is a minimal example of defining an `Embedding` layer:\n",
        "```python\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,     # size of the vocabulary\n",
        "    output_dim=128,           # embedding vector dimension\n",
        "    input_length=sequence_length\n",
        ")\n",
        "```\n",
        "- This layer transforms integer-encoded sequences (word IDs) into dense vector embeddings.\n",
        "\n",
        "- Feed these embeddings into your LSTM or GRU OR 1D CNN layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7c8bUB5pzioY"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "sequence_length = window_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "OXCK40l6MRld"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,     # size of the vocabulary\n",
        "    output_dim=128,           # embedding vector dimension\n",
        "    input_length=6\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsXR4RZpMXMi"
      },
      "source": [
        "## 4. Model\n",
        "- Implement an LSTM or GRU or 1D CNN-based language model with:\n",
        "  - **The Embedding layer** as input.\n",
        "  - At least **one recurrent layer** (e.g., `LSTM(256)` or `GRU(256)` or your custom 1D CNN).\n",
        "  - A **Dense** output layer with **softmax** activation for word prediction.\n",
        "- Train for about **5–10 epochs** so it can finish in approximately **2 hours** on a standard machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "4jZyNlmlz1Cc",
        "outputId": "e6f89004-a7f6-4e90-8854-f8a2ca737da0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "es = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_weights.keras',\n",
        "                                      save_best_only=True,\n",
        "                                      monitor='val_accuracy',\n",
        "                                      mode='max',\n",
        "                                      verbose=1)\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggop4h4IMhMT"
      },
      "source": [
        "## 5. Training & Evaluation\n",
        "- **Monitor** the loss on both training and validation sets.\n",
        "- **Perplexity**: a common metric for language models.\n",
        "  - It is the exponent of the average negative log-likelihood.\n",
        "  - If your model outputs cross-entropy loss `H`, then `perplexity = e^H`.\n",
        "  - Try to keep the validation perplexity **under 50** if possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "P8d8FS2XMj46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5de0e89-8848-4e94-b2d7-f62a06d0247e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.0554 - loss: 7.2909\n",
            "Epoch 1: val_accuracy improved from -inf to 0.10097, saving model to best_weights.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.0556 - loss: 7.2802 - val_accuracy: 0.1010 - val_loss: 5.7202\n",
            "Epoch 2/5\n",
            "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.0736 - loss: 6.2128\n",
            "Epoch 2: val_accuracy improved from 0.10097 to 0.11449, saving model to best_weights.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 118ms/step - accuracy: 0.0737 - loss: 6.2137 - val_accuracy: 0.1145 - val_loss: 5.6534\n",
            "Epoch 3/5\n",
            "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.0886 - loss: 6.0974\n",
            "Epoch 3: val_accuracy improved from 0.11449 to 0.11787, saving model to best_weights.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 110ms/step - accuracy: 0.0888 - loss: 6.0972 - val_accuracy: 0.1179 - val_loss: 5.6382\n",
            "Epoch 4/5\n",
            "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.0996 - loss: 5.9657\n",
            "Epoch 4: val_accuracy improved from 0.11787 to 0.12271, saving model to best_weights.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 119ms/step - accuracy: 0.0998 - loss: 5.9651 - val_accuracy: 0.1227 - val_loss: 5.6657\n",
            "Epoch 5/5\n",
            "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.1199 - loss: 5.8040\n",
            "Epoch 5: val_accuracy improved from 0.12271 to 0.12512, saving model to best_weights.keras\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - accuracy: 0.1199 - loss: 5.8040 - val_accuracy: 0.1251 - val_loss: 5.6130\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.1245 - loss: 5.6234\n",
            "Val Perplexity:  273.96540065289906\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "network_history = model.fit(X_train, Y_train,\n",
        "                            validation_data=(X_val,Y_val),\n",
        "                            batch_size=128,\n",
        "                            epochs=5,\n",
        "                            verbose=1,\n",
        "                            callbacks=[es, checkpoint])\n",
        "\n",
        "val_loss, val_acc = model.evaluate(X_val, Y_val)\n",
        "\n",
        "print(\"Val Perplexity: \", np.exp(val_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbvbBOp3MfTD"
      },
      "source": [
        "## 6. Generation Criteria\n",
        "- After training, generate **two distinct text samples**, each at least **50 tokens**.\n",
        "- Use **different seed phrases** (e.g., “love is” vs. “time will”)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1uHjn6aHMW5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71435ca0-b2bc-41e9-97c3-d5b5c7aa5eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text 1:\n",
            "love is union rapid palings brief cricket raves of places his melody at cover on to importance of the hush that taken tied was the descended of the lasses forms the procure with the told of given their occasional jolly of sleepy reasoners as the pensive erudition he his direful dismay eloped\n",
            "\n",
            "Generated Text 2:\n",
            "time will run unimaginable armed arising knightserrant vocal hessian a close stubble retreats of shrub scarlet every course and beaming into then made been esteemed of winced half a time who the expanded of the overturned which lonely the cheerily and him was i belly of a houten by the pipe the\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Load the best weights\n",
        "model.load_weights('best_weights.keras')\n",
        "\n",
        "def generate_text(seed_text, next_words=50):\n",
        "    # Ensure seed_text is a list of tokens\n",
        "    if isinstance(seed_text, str):\n",
        "        seed_text = seed_text.lower().split()\n",
        "\n",
        "    for _ in range(next_words):\n",
        "        token_list = seed_text[-sequence_length:]\n",
        "        token_ids = [vocab.get(token, 0) for token in token_list]\n",
        "\n",
        "        # Pad if needed\n",
        "        if len(token_ids) < sequence_length:\n",
        "            token_ids = [0] * (sequence_length - len(token_ids)) + token_ids\n",
        "\n",
        "        token_ids = np.array([token_ids])\n",
        "        predicted_probs = model.predict(token_ids, verbose=0)[0]\n",
        "\n",
        "        # Sample instead of taking the argmax\n",
        "        predicted_id = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
        "\n",
        "        # Find word for predicted id\n",
        "        output_word = next((word for word, idx in vocab.items() if idx == predicted_id), \"\")\n",
        "        seed_text.append(output_word)\n",
        "\n",
        "    return ' '.join(seed_text)\n",
        "\n",
        "# Generate two text samples with different seed phrases\n",
        "seed_phrase1 = \"love is\"\n",
        "generated_text1 = generate_text(seed_phrase1)\n",
        "print(f\"Generated Text 1:\\n{generated_text1}\")\n",
        "\n",
        "seed_phrase2 = \"time will\"\n",
        "generated_text2 = generate_text(seed_phrase2)\n",
        "print(f\"\\nGenerated Text 2:\\n{generated_text2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5CpdqF9MoPj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}